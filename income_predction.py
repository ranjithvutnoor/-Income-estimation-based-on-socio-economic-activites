# -*- coding: utf-8 -*-
"""income_predction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sl-1eMNwVj3nb_Ule7eWQgZn0IXsrs_r

# doing the necessary imports
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

"""# reading the data file"""

df =pd.read_csv('incomeData.csv')
df.head()

df.tail()

"""# Making sure that the data doesn't cintain unecessary spaces"""

df = df.apply(lambda x : x.str.strip() if x.dtype=='object' else x)

df.head(50)

df.info()

df.describe

"""# In this dataset missing values have been denoted by '?'
# we are replacing ? with NaN for them to be imputed down the line.
"""

df.replace('?',np.NaN,inplace=True)

df.head(50)

df.isna().sum()

df.isna()

"""# checking for the missing values"""

cols = df.columns
ans=[]
null_sum = df.isna().sum()
for i in range(15):
  if null_sum[i]>0:
    ans.append(cols[i])
print(ans)

cols = df.columns
ans=[]
null_sum = df.isna().sum()
for i in range(15):
  if null_sum[i]==0:
    ans.append(cols[i])
print(ans)
print(len(ans))

df.info()

"""# As the columns which have missing values, they are only categorical, we'll use the categorical imputer
# Importing the categorical imputer
"""

from sklearn_pandas import CategoricalImputer
imputer = CategoricalImputer(missing_values='NaN',strategy='most_frequent')

"""# imputing the missing values from the column"""

df['workclass'] = imputer.fit_transform(df['workclass'])
df['occupation'] = imputer.fit_transform(df['occupation'])
df['native-country'] = imputer.fit_transform(df['native-country'])

df.isna().sum()

df.drop(columns=['education'],inplace=True)

cate_df = df.select_dtypes(include=['object']).copy()

cate_df.columns,len(cate_df.columns)

inte_df = df.select_dtypes(include=['int64']).copy()
inte_df.columns,len(inte_df.columns)

cate_df.head()

inte_df.head()

cate_df['Income'].unique()

cate_df['native-country'].unique()

cate_df['marital-status'].unique()

cate_df['workclass'].unique()

cate_df.drop('Income',axis=1)

Income = cate_df['Income']=cate_df['Income'].map({'<=50K':0, '>50K':1})

"""# Using the onehot encoding to encode the categorical columns to numericsl ones"""

from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(categories='auto', drop=None, sparse=False)
encoded_data = ohe.fit_transform(cate_df)

encoded_data.shape

encoded_data1 =pd.DataFrame(encoded_data)
encoded_data1.head()

"""# extracting the numerical columns"""

from sklearn.preprocessing import StandardScaler

scaler=StandardScaler()
scaled_data=scaler.fit_transform(inte_df)

scaled_num_df= pd.DataFrame(data=scaled_data, columns=inte_df.columns)

scaled_num_df.head()

"""# combining the Numerical and categorical dataframes to get the final dataset"""

final_df=pd.concat([scaled_num_df,encoded_data1], axis=1)

final_df['Income'] = Income

final_df.shape

final_df.head()

x=final_df.drop('Income',axis=1)
y=final_df['Income']

"""If we plot the distribution of the target column, we'd find that the peole with less than 50K annual income are more in number than the people with an annual income greaterthan 50K"""

x.shape

y.shape

plt.hist(y)

"""Hence, the dataset is imbalanced. we need to introduce some random sampling to make it balanced."""

!pip install imbalanced-learn

from imblearn.over_sampling import RandomOverSampler

rdsmple = RandomOverSampler(sampling_strategy='auto',random_state=None, shrinkage=None)
x_sampled,y_sampled  = rdsmple.fit_resample(x,y)

"""# again plotting the target column"""

plt.hist(y_sampled)

"""As shown above, now the data looks to be balanced.

# splitting the data into training and test set
"""

from sklearn.model_selection import train_test_split
train_x,test_x,train_y,test_y=train_test_split(x_sampled,y_sampled,test_size=0.25,random_state=355)

from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import CategoricalNB
gnb = GaussianNB(priors=None, var_smoothing=0.05)

y_pred = gnb.fit(train_x, train_y).predict(test_x)

from sklearn.metrics import accuracy_score

sc=accuracy_score(test_y,y_pred)
sc

from sklearn.model_selection import GridSearchCV

param_grid = {"var_smoothing": [1e-9,0.1, 0.001, 0.5,0.05,0.01,1e-8,1e-7,1e-6,1e-10,1e-11]}

grid = GridSearchCV(estimator=gnb, param_grid=param_grid, cv=5,  verbose=3)

grid.fit(train_x, train_y)

grid.best_estimator_

from xgboost import XGBClassifier

xgb=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, criterion='gini', gamma=0,
              learning_rate=0.1, max_delta_step=0, max_depth=9,
              min_child_weight=1, missing=None, n_estimators=130, n_jobs=1,
              nthread=None, objective='binary:logistic', random_state=0,
              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
              silent=None, subsample=1, verbosity=1)

y_pred = xgb.fit(train_x, train_y).predict(test_x)

ac2=accuracy_score(test_y,y_pred)
ac2

param_grid = {"n_estimators": [10, 50, 100, 130], "criterion": ['gini', 'entropy'],
                               "max_depth": range(2, 10, 1)}

            #Creating an object of the Grid Search class
grid = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5,  verbose=3,n_jobs=-1)

#finding the best parameters
grid.fit(train_x, train_y)

grid.best_estimator_

xgb=XGBClassifier(criterion='gini', max_depth=2, n_estimators=10)

y_pred = xgb.fit(train_x, train_y).predict(test_x)

ac2=accuracy_score(test_y,y_pred)
ac2

